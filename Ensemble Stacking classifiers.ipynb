{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/learn-together/train.csv\", index_col='Id')\ntest = pd.read_csv(\"../input/learn-together/test.csv\",index_col='Id')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imputing Hillshade_3pm zeros as per https://www.kaggle.com/arateris/stacked-classifiers-for-forest-cover and also checking the correlations with Hillshade_9am and Hillshade_3pm to find the features need to impute. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# questionable_0 = ['Hillshade_9am', 'Hillshade_3pm']\n# all_data = train.append(test)\n# for col in questionable_0:\n#     all_data_0 = all_data[all_data[col] == 0].copy()\n#     all_data_non0 = all_data[all_data[col] != 0].copy()\n#     corr = all_data_non0.corr()[col]\n#     corr = np.abs(corr[corr.index != col]).sort_values(ascending = False)\n    \n#     sns.set_style('whitegrid')\n#     plt.figure(figsize = (17,4))\n#     fig = sns.barplot(x=corr.index, y=corr)\n#     fig.set_xticklabels(labels = corr.index, rotation=90)\n#     plt.ylabel('Correlation')\n#     plt.title(col)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see Aspect, Slope, Hillshade_9am, Hillshade_Noon are higly correlated with Hillshade_3pm. Going to impute this variable alone for this iteration,Used https://www.kaggle.com/hoangnguyen719/part-1-eda-and-feature-engineering to plot these correlation graphs. Will consider imputing Hillshade_9am variable in the next iteration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr = train.corr()['Slope']\n# corr.style.background_gradient(cmap='coolwarm')\n\n#replacing the zeros for better guess, mainly to avoid zeros in the feature engineering and fake outliers. \nnum_train=len(train)\nall_data = train.append(test)\ncols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\nHS_zero = all_data[all_data.Hillshade_3pm==0]\nHS_zero.shape\n\nHS_train = all_data[all_data.Hillshade_3pm!=0]\n# res = cross_val_score(RandomForestRegressor(n_estimators=100), HS_train.drop('Hillshade_3pm',axis=1), HS_train.Hillshade_3pm, n_jobs=-1, verbose=True)\n# print(res) #[0.9996774  0.99989463 0.9999186 ]\n##actually, the CV is so close to zero there is actually no new information here..keeping it for simplicity\nrf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\nout = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\nall_data.loc[HS_zero.index,'Hillshade_3pm'] = out\ntrain= all_data[:num_train]\ntest= all_data[num_train:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reused the features https://www.kaggle.com/jakelj/basic-ensemble-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['EV_DTH'] = (train.Elevation - train.Vertical_Distance_To_Hydrology)\ntest['EV_DTH'] = (test.Elevation - test.Vertical_Distance_To_Hydrology)\n\n\ntrain['EH_DTH'] = (train.Elevation -  (train.Horizontal_Distance_To_Hydrology *0.2))\ntest['EH_DTH'] = (test.Elevation -  (test.Horizontal_Distance_To_Hydrology *0.2))\n\ntrain['Dis_To_Hy'] = (((train.Horizontal_Distance_To_Hydrology **2) + (train.Vertical_Distance_To_Hydrology **2))**0.5)\ntest['Dis_To_Hy'] = (((test.Horizontal_Distance_To_Hydrology **2) + (test.Vertical_Distance_To_Hydrology **2))**0.5)\n\ntrain['HyF_1'] = (train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Fire_Points)\ntest['HyF_1'] = (test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Fire_Points)\n\ntrain['HyF_2'] = (train.Horizontal_Distance_To_Hydrology - train.Horizontal_Distance_To_Fire_Points)\ntest['HyF_2'] = (test.Horizontal_Distance_To_Hydrology - test.Horizontal_Distance_To_Fire_Points)\n\ntrain['HyR_1'] = (train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways)\ntest['HyR_1'] = (test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways)\n\ntrain['HyR_2'] = (train.Horizontal_Distance_To_Hydrology - train.Horizontal_Distance_To_Roadways)\ntest['HyR_2'] = (test.Horizontal_Distance_To_Hydrology - test.Horizontal_Distance_To_Roadways)\n\n\ntrain['FiR_1'] = (train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Roadways)\ntest['FiR_1'] = (test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Roadways)\n\ntrain['FiR_1'] = (train.Horizontal_Distance_To_Fire_Points - train.Horizontal_Distance_To_Roadways)\ntest['FiR_1'] = (test.Horizontal_Distance_To_Fire_Points - test.Horizontal_Distance_To_Roadways)\n\ntrain['Avg_shade'] = ((train.Hillshade_9am + train.Hillshade_Noon + train.Hillshade_3pm) /3)\ntest['Avg_shade'] = ((test.Hillshade_9am + test.Hillshade_Noon + test.Hillshade_3pm) /3)\n\ntrain['Morn_noon_int'] = ((train.Hillshade_9am + train.Hillshade_Noon) / 2)\ntest['Morn_noon_int'] = ((test.Hillshade_9am + test.Hillshade_Noon) / 2)\n\ntrain['noon_eve_int'] = ((train.Hillshade_3pm + train.Hillshade_Noon) / 2)\ntest['noon_eve_int'] = ((test.Hillshade_3pm + test.Hillshade_Noon) / 2)\n\ntrain['Slope2'] = np.sqrt(train.Horizontal_Distance_To_Hydrology**2 + train.Vertical_Distance_To_Hydrology**2)\ntest['Slope2'] = np.sqrt(test.Horizontal_Distance_To_Hydrology**2 + test.Vertical_Distance_To_Hydrology**2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing features from this notebook https://www.kaggle.com/justfor/ensembling-and-stacking-with-heamy in search for a better defined domain knowledge"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = add_feats(train)    \n# test = add_feats(test)  \n# covertype_labelcolumn = train['Cover_Type']\n# train.drop(['Soil_Type15', 'Soil_Type7','Cover_Type'], axis=1, inplace = True)\nlabel = train['Cover_Type']\ntrain.drop(['Soil_Type15', 'Soil_Type7','Cover_Type'], axis=1, inplace = True)\ntest.drop(['Soil_Type15', 'Soil_Type7','Cover_Type'], axis=1, inplace = True)\nall_data=train.append(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# questionable_0 = ['Hillshade_9am', 'Hillshade_3pm']\n\n# for col in questionable_0:\n#     all_data_0 = all_data[all_data[col] == 0].copy()\n#     all_data_non0 = all_data[all_data[col] != 0].copy()\n#     corr = all_data_non0.corr()[col]\n#     corr = np.abs(corr[corr.index != col]).sort_values(ascending = False)\n    \n#     sns.set_style('whitegrid')\n#     plt.figure(figsize = (17,4))\n#     fig = sns.barplot(x=corr.index, y=corr)\n#     fig.set_xticklabels(labels = corr.index, rotation=90)\n#     plt.ylabel('Correlation')\n#     plt.title(col)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_train=len(train.drop(['Cover_Type'], axis =1))\n# print (n_train)\nn_train = len(test.index)\nprint (n_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding PCA components from the entire raw features . testing the PCA code from https://www.kaggle.com/edumunozsala/feature-eng-and-a-simple-stacked-model to see if the PCA increase the performance of the algorithm. Also ignore Gaussian Mixture modeling in this run"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.decomposition import PCA\n    \n# #Apply PCA to obtain new features based in all datasets (train and test)\n# def add_PCA_features(X):\n#     pca = PCA(n_components=0.99).fit(X)\n#     X_pca = pca.transform(X)\n    \n#     return X_pca\n\n# components = add_PCA_features(all_data)\n# # components = add_PCA_features(train)\n# # print (train.shape)\n# # print (test.shape)\n# # print (all_data.shape)\n# # print (n_train)\n\n\n# print('PCA components dimension: ',components.shape)\n# # print (components)\n# for i in range(components.shape[1]):\n#     col_name= 'pca'+str(i+1)\n#     test[col_name] = components[:n_train, i]\n#     train[col_name] = components[n_train:, i]\n\n# print(test.shape,train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test data being so large compared to Train, using GaussianMixture Modeling fit on test data and using the data categorized into clusters as an added feature to train and test data sets. Reused from https://www.kaggle.com/stevegreenau/stacking-multiple-classifiers-clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.mixture import GaussianMixture\n\n# gmix = GaussianMixture(n_components=10)\n# gmix.fit(test)\n\n# train['Test_Cluster'] = gmix.predict(train.drop(['Cover_Type'], axis =1))\n# test['Test_Cluster'] = gmix.predict(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train, label, test_size=0.2, random_state = 2)\n# X_train, X_val, y_train, y_val = train_test_split(train.drop(['Cover_Type'], axis =1), train['Cover_Type'], test_size=0.2, random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using L1 regularization to select the most telling features . on Training data and keep only the features selected by L1 regularization. Also drop unselected columns from validation and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.feature_selection import SelectFromModel\n\n# X, y = X_train, y_train\n\n# \"\"\" Return selected features using logistic regression with an L1 penalty \"\"\"\n# logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7).fit(X, y)\n# model = SelectFromModel(logistic, prefit=True)\n# X_new = model.transform(X)\n# selected_features = pd.DataFrame(model.inverse_transform(X_new), \n#                              index=X.index,\n#                              columns=X.columns)\n\n# selected_columns = selected_features.columns[selected_features.var() != 0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(X_train.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = X_train[selected_columns]\n# X_val = X_val[selected_columns]\n# len(X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(test.columns)\n# # test = test[selected_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use pandas profiling to get a report on EDA views"},{"metadata":{},"cell_type":"markdown","source":"Let's delete the Id column in the training set but store it for the test set before deleting"},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{},"cell_type":"markdown","source":"Let's use 80% of the Data for training, and 20% for validation. We'll then train a simple Random Forest Classifier with 100 trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.model_selection import cross_val_score,cross_validate, train_test_split, GridSearchCV, StratifiedKFold\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_val, y_train, y_val = train_test_split(train.drop(['Cover_Type'], axis=1), train['Cover_Type'], test_size=0.2, random_state = 42)\n# X_train, X_val, y_train, y_val = train_test_split(train.drop(['Cover_Type'], axis =1), train['Cover_Type'], test_size=0.2, random_state = 25)\n# random_state = 1\n# X_train, X_val, y_train, y_val = train_test_split(train,covertype_labelcolumn, test_size=0.2, random_state = random_state)\n# X_train, X_val, y_train, y_val = train_test_split(train.drop(['Cover_Type'], axis =1), train['Cover_Type'], test_size=0.2, random_state = 2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Has been using boosting all this while, testing stacking the classifiers on the same features,  credits to https://www.kaggle.com/kwabenantim/forest-cover-stacking-multiple-classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = train.drop(['Cover_Type'], axis =1)\n# y_train = train['Cover_Type']\n\nprint('> Setting up classifiers')\nrandom_state = 1\nmax_features = min(30, X_train.columns.size)\n\n\nab_clf = AdaBoostClassifier(n_estimators=200,\n                            base_estimator=DecisionTreeClassifier(\n                                min_samples_leaf=2,\n                                random_state=random_state),\n                            random_state=random_state)\n\net_clf = ExtraTreesClassifier(n_estimators=300,\n                              min_samples_leaf=2,\n                              min_samples_split=2,\n                              max_depth=50,\n                              max_features=max_features,\n                              random_state=random_state,\n                              n_jobs=1)\n\nlg_clf = LGBMClassifier(n_estimators=300,\n                        num_leaves=128,\n                        verbose=-1,\n                        random_state=random_state,\n                        n_jobs=1)\n\nrf_clf = RandomForestClassifier(n_estimators=300,\n                                random_state=random_state,\n                                n_jobs=1)\n\nensemble = [('AdaBoostClassifier', ab_clf),\n            ('ExtraTreesClassifier', et_clf),\n            ('LGBMClassifier', lg_clf),\n            ('RandomForestClassifier', rf_clf)]\n\n\nprint('> Cross-validating classifiers')\nfor label, clf in ensemble:\n    score = cross_val_score(clf, X_train, y_train,\n                            cv=5,\n                            scoring='accuracy',\n                            verbose=0,\n                            n_jobs=-1)\n\n    print('  -- {: <24} : {:.3f} : {}'.format(label, np.mean(score), np.around(score, 3)))\n\n\nprint('> Fitting stack')\n# - https://www.kaggle.com/itslek/stack-blend-lrs-xgb-lgb-house-prices-k-v17\n# - https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n# - https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n\nstack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, rf_clf],\n                             meta_classifier=rf_clf,\n                             cv=5,\n                             stratify=True,\n                             shuffle=True,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=random_state,\n                             n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n# low_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and \n# #                         X_train[cname].dtype == \"object\"]\n# low_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() == 2 ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = RandomForestClassifier(n_estimators=1000, max_depth=10)\n# model = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=5)\n# model = XGBClassifier(n_estimators=1000, learning_rate=0.1, max_depth=10)\n# model = XGBClassifier()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# param = {'n_estimators': [100, 500,1000],\n#          'learning_rate': [0.1, 0.05],\n#          'max_depth': [3, 4, 5, 6]}\n# grider = GridSearchCV(model, param, n_jobs=-1, cv=5, scoring='accuracy', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack.fit(X_train, y_train)\n# model.fit(X_train, y_train)\n# grider.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print (grider.best_params_)\n# print(grider.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_pred = model.predict(X_train)\nstack.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.score(X_train, y_train)\n# train_pred = model.predict(X_train)\n# mae_1 = mean_absolute_error(train_pred, y_train) # Your code here\n\n# # Uncomment to print MAE\n# print(\"Mean Absolute Error:\" , str(mae_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = XGBClassifier(n_estimators=100)\n# model.fit(X_train, y_train)\n# # make predictions for test data\n# y_pred = model.predict(X_val)\n# predictions = [round(value) for value in y_pred]\n# # evaluate predictions\n# accuracy = accuracy_score(y_val, predictions)\n# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = stack.predict(X_val)\n# # mean_absolute_error(predictions, y_val)\naccuracy_score(y_val, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model has a 100% accuracy on the training set and ~ 89.947% on the test set. XGboost classifier instead of randomforest and some feature engineering (added a few features inspired from earlier work)\n\nAdded PCA components in addition to the features, training accuracy 99.99917% and 89.087301% on test set.\n\nRemoved PCA but imputing Hillshade_3pm \"0\" values using Randomforestregressior as mentioned in https://www.kaggle.com/arateris/stacked-classifiers-for-forest-cover , training accuracy 100% and 90.244% on test set."},{"metadata":{},"cell_type":"markdown","source":"# Final Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test.head()\n\nprint('Missing test data? ', test.isnull().any().any())\n# print(test.columns[test.isnull().any()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = stack.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'ID': test.index,\n                       'Cover_Type': test_pred})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Planned next steps\n\n* Feature Extraction/ Feature engineering\n* Introduce Regularization techniques\n* Hyper parameter tuning - Grid-Search to find the optimal parameters for our classifier \n* Xgboost classifier\n\nStacking multiple classifiers on the same features provided a test accuracy of 89.35% and train accuracy of 99%"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}